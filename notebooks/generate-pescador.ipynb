{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import random\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "from IPython.display import Audio\n",
    "import pescador\n",
    "import os\n",
    "import random,librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_x and pos_y are integers from 1 to 9; snr_ratio should be from 0 to 1\n",
    "def generate_examples(speech_path,noise_dir,srir_dir,pos_x,pos_y,d_yaw,d_pitch,d_roll,snr_ratio):\n",
    "    #load in mono speech\n",
    "    src_audio, sr = librosa.load(speech_path, sr=44100, mono=True)\n",
    "    src_audio /= src_audio.max()\n",
    "    \n",
    "    #convolve speech with srir\n",
    "    grid_x=pos_x\n",
    "    grid_y=pos_y\n",
    "    \n",
    "    ch_out_list = []\n",
    "    sh_names = [\"W\", \"X\", \"Y\", \"Z\"]\n",
    "    for sh_str in sh_names:\n",
    "        ch_ir_path = os.path.join(srir_dir, sh_str,\n",
    "                              \"{}x{:02d}y{:02d}.wav\".format(sh_str, grid_x, grid_y))\n",
    "        ch_ir, sr = librosa.load(srir_dir, sr=44100)\n",
    "        \n",
    "        ch_ir_len = ch_ir.shape[0]\n",
    "        src_len = src_audio.shape[0]\n",
    "    \n",
    "        if ch_ir_len > src_len:\n",
    "            pad_len = ch_ir_len - src_len\n",
    "            src_audio = np.pad(src_audio, (0, pad_len), mode='constant')\n",
    "        elif ch_ir_len < src_len:\n",
    "            pad_len = src_len - ch_ir_len\n",
    "            ch_ir = np.pad(ch_ir, (0, pad_len), mode='constant')\n",
    "        \n",
    "        ch_out = scipy.signal.fftconvolve(src_audio, ch_ir, mode='full')[:src_len]\n",
    "        ch_out_list.append(ch_out)\n",
    "\n",
    "    src_bformat = np.array(ch_out_list)\n",
    "    \n",
    "    #load in bformat noise\n",
    "    noise_data = None\n",
    "    while noise_data is None or noise_data.shape[1] < src_bformat.shape[1]:\n",
    "        noise_data, sr = librosa.load(noise_dir, sr=44100, mono=False)\n",
    "    \n",
    "    #align bformat noise and bformat speech\n",
    "    clip_len = src_bformat.shape[1]\n",
    "    start_idx = np.random.randint(0, noise_data.shape[1] - clip_len)\n",
    "    noise_data = noise_data[:,start_idx:start_idx + clip_len]\n",
    "    \n",
    "    #designate snr and scale\n",
    "    snr = 10 * np.log10(np.mean(src_bformat[0,:] ** 2) / np.mean(noise_data[0,:] ** 2))\n",
    "    snr_target = snr_ratio * 40.0 - 20.0\n",
    "    alpha = 10.0**((snr_target - snr) / 20.0)#scaling factor\n",
    "    src_bformat *= alpha\n",
    "    \n",
    "    #combine the noise+speech\n",
    "    mix_bformat = src_bformat + noise_data\n",
    "    \n",
    "    return mix_bformat\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_x and pos_y are integers from 1 to 9; snr_ratio should be from 0 to 1\n",
    "def generate_examples(speech_path,noise_dir,srir_dir,pos_x,pos_y,d_yaw,d_pitch,d_roll,snr_ratio):\n",
    "    #load in mono speech\n",
    "    src_audio, sr = librosa.load(speech_path, sr=44100, mono=True)\n",
    "    src_audio /= src_audio.max()\n",
    "    \n",
    "    #convolve speech with srir\n",
    "    grid_x=pos_x\n",
    "    grid_y=pos_y\n",
    "    \n",
    "    ch_out_list = []\n",
    "    sh_names = [\"W\", \"X\", \"Y\", \"Z\"]\n",
    "    for sh_str in sh_names:\n",
    "        ch_ir_path = os.path.join(srir_dir, sh_str,\n",
    "                              \"{}x{:02d}y{:02d}.wav\".format(sh_str, grid_x, grid_y))\n",
    "        ch_ir, sr = librosa.load(srir_dir, sr=44100)\n",
    "        \n",
    "        ch_ir_len = ch_ir.shape[0]\n",
    "        src_len = src_audio.shape[0]\n",
    "    \n",
    "        if ch_ir_len > src_len:\n",
    "            pad_len = ch_ir_len - src_len\n",
    "            src_audio = np.pad(src_audio, (0, pad_len), mode='constant')\n",
    "        elif ch_ir_len < src_len:\n",
    "            pad_len = src_len - ch_ir_len\n",
    "            ch_ir = np.pad(ch_ir, (0, pad_len), mode='constant')\n",
    "        \n",
    "        ch_out = scipy.signal.fftconvolve(src_audio, ch_ir, mode='full')[:src_len]\n",
    "        ch_out_list.append(ch_out)\n",
    "\n",
    "    src_bformat = np.array(ch_out_list)\n",
    "    \n",
    "    #load in bformat noise\n",
    "    noise_data = None\n",
    "    while noise_data is None or noise_data.shape[1] < src_bformat.shape[1]:\n",
    "        noise_data, sr = librosa.load(noise_dir, sr=44100, mono=False)\n",
    "    \n",
    "    #align bformat noise and bformat speech\n",
    "    clip_len = src_bformat.shape[1]\n",
    "    start_idx = np.random.randint(0, noise_data.shape[1] - clip_len)\n",
    "    noise_data = noise_data[:,start_idx:start_idx + clip_len]\n",
    "    \n",
    "    #designate snr and scale\n",
    "    snr = 10 * np.log10(np.mean(src_bformat[0,:] ** 2) / np.mean(noise_data[0,:] ** 2))\n",
    "    snr_target = snr_ratio * 40.0 - 20.0\n",
    "    alpha = 10.0**((snr_target - snr) / 20.0)#scaling factor\n",
    "    src_bformat *= alpha\n",
    "    \n",
    "    #combine the noise+speech\n",
    "    mix_bformat = src_bformat + noise_data\n",
    "    \n",
    "    return src_bformat, noise_data, mix_bformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_speech_mask_sampler(speech_path, noise_dir, srir_dir, sc_to_pos_dict):\n",
    "    sc_list = sorted(sorted(list(sc_to_pos_dict.keys()), key=lambda x: x[1]), key=lambda x: x[0])\n",
    "    azi_list, elv_list = zip(*sc_list)\n",
    "    steer_mat = steer_vector(azi_list, elv_list)\n",
    "\n",
    "    while True:\n",
    "        steer_idx = np.random.randint(len(sc_list))\n",
    "        azi, elv = azi_list[steer_idx], elv_list[steer_idx]\n",
    "        pos_x, pos_y, d_yaw, d_pitch, d_roll = random.choice(sc_to_pos_dict[(azi, elv)])\n",
    "\n",
    "        snr_ratio = np.random.random()\n",
    "\n",
    "        src, noise, mix = generate_examples(speech_path,noise_dir,srir_dir,pos_x,pos_y,d_yaw,d_pitch,d_roll,snr_ratio)\n",
    "        inp = featurematrix(steer_idx, mix, steer_mat)\n",
    "\n",
    "        mask, _ = compute_masks(src[0], noise[0])\n",
    "        \n",
    "        yield {\n",
    "            'input': inp,\n",
    "            'mask': mask\n",
    "        }\n",
    "\n",
    "def lstm_data_generator(speech_dir, noise_dir, srir_dir, sc_to_pos_dict, batch_size, active_streamers,\n",
    "                        rate, random_state=12345678):\n",
    "    for root, dir_list, file_list in os.walk(speech_dir):\n",
    "        for fname in file_list:\n",
    "            if not fname.endswith('.wav'):\n",
    "                continue\n",
    "\n",
    "            speech_path = os.path.join(root, fname)\n",
    "\n",
    "            streamer = pescador.Streamer(lstm_speech_mask_sampler, \n",
    "                                         speech_path, noise_dir, srir_dir, sc_to_pos_dict)\n",
    "            seeds.append(streamer)\n",
    "\n",
    "    # Randomly shuffle the seeds\n",
    "    random.shuffle(seeds)\n",
    "\n",
    "    mux = pescador.StochasticMux(seeds, active_streamers, rate=rate, random_state=random_state)\n",
    "\n",
    "    if batch_size == 1:\n",
    "        return mux\n",
    "    else:\n",
    "        return pescador.maps.buffer_stream(mux, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jtc440/dev/3daudio/ambisonic_source_separation/isophonics/greathall/Z/Zx00y03.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-93271b7db983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpos_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msnr_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmix_bformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrir_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnr_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d56aad302e6f>\u001b[0m in \u001b[0;36mgenerate_examples\u001b[0;34m(speech_dir, noise_dir, srir_dir, pos_x, pos_y, snr_ratio)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#ch_ir_path = os.path.join(srir_dir, sh_str,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#                      \"{}x{:02d}y{:02d}.wav\".format(sh_str, grid_x, grid_y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mch_ir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrir_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mch_ir_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch_ir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/beegfs/jtc440/miniconda3/lib/python3.6/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/beegfs/jtc440/miniconda3/lib/python3.6/site-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrawread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrawread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawAudioFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/beegfs/jtc440/miniconda3/lib/python3.6/site-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jtc440/dev/3daudio/ambisonic_source_separation/isophonics/greathall/Z/Zx00y03.wav'"
     ]
    }
   ],
   "source": [
    "speech_dir='./vctk-p225/p225_003.wav'\n",
    "noise_dir = './ambiencelondonstreet.wav'\n",
    "srir_dir='./isophonics/greathall'\n",
    "pos_x=0\n",
    "pos_y=0\n",
    "snr_ratio=0.1\n",
    "mix_bformat=generate_examples(speech_dir,noise_dir,srir_dir,pos_x,pos_y,snr_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_90(audio):\n",
    "    return scipy.signal.hilbert(audio).imag\n",
    "sr=44100\n",
    "# Mix to stereo according to https://en.wikipedia.org/wiki/Ambisonic_UHJ_format#UHJ_encoding_and_decoding_equations\n",
    "# S = 0.9396926*W + 0.1855740*X\n",
    "# D = j(-0.3420201*W + 0.5098604*X) + 0.6554516*Y\n",
    "# Left = (S + D)/2.0\n",
    "# Right = (S - D)/2.0\n",
    "S = 0.9396926 * mix_bformat[0] + 0.1855740 * mix_bformat[1]\n",
    "D = rotate_90(-0.3420201 * mix_bformat[0] + 0.5098604 * mix_bformat[1]) + 0.6554516 * mix_bformat[2]\n",
    "L = (S + D)/2.0\n",
    "R = (S - D)/2.0\n",
    "mix_mono = S\n",
    "mix_stereo = np.stack([L,R])\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(mix_stereo,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature extraction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. steering matrix\n",
    "2. beamformer (pseudo-inverse of steering matrix)\n",
    "3. concatenate s_hat, n_hat, x_w to be the feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get azimuth and elevation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we swap x and y here to be consistent with standard \n",
    "# spherical coordinate convention\n",
    "speaker_coord = np.array([-2, 6, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical(dist_coord, rads=True):\n",
    "    # Convert cartesian coordiantes to spherical coordinates\n",
    "    x, y, z = dist_coord\n",
    "    r = np.linalg.norm(dist_coord)\n",
    "    azi = math.atan2(y, x)\n",
    "    # NOTE: This differs from typical elevation/inclination, which is usuall measured with 0 pointing up\n",
    "    elv = math.asin(z/float(r))\n",
    "    \n",
    "    if elv > np.pi/2:                    \n",
    "        elv = (np.pi - elv) % (np.pi/2)\n",
    "        azi = (np.pi + azi) % (2 * np.pi) - np.pi\n",
    "    elif elv < - np.pi/2:\n",
    "        elv = -((np.pi + elv) % (np.pi/2))\n",
    "        azi = (np.pi + azi) % (2 * np.pi) - np.pi\n",
    "\n",
    "    \n",
    "    if not rads:\n",
    "        azi = azi * 180.0 / np.pi\n",
    "        elv = elv * 180.0 / np.pi\n",
    "        \n",
    "    return np.array([r, azi, elv])\n",
    "\n",
    "# https://github.com/polarch/Spherical-Harmonic-Transform/blob/master/euler2rotationMatrix.m\n",
    "def euler_to_rotation_matrix(alpha, beta, gamma, order='xyz'):\n",
    "    \"\"\"\n",
    "    %   alpha:  first angle of rotation\n",
    "    %   beta:   second angle of rotation\n",
    "    %   gamma:  third angle of rotation\n",
    "    %\n",
    "    %   order:  definition of the axes of rotation, e.g. for the y-convention\n",
    "    %           this should be 'zyz', for the x-convnention 'zxz', and for\n",
    "    %           the yaw-pitch-roll convention 'zyx'\n",
    "    \"\"\"\n",
    "    def Rx(theta):\n",
    "        return np.array([\n",
    "            [1.0, 0.0, 0.0],\n",
    "            [0.0, np.cos(theta), np.sin(theta)],\n",
    "            [0.0, -np.sin(theta), np.cos(theta)]])\n",
    "    \n",
    "    def Ry(theta):\n",
    "        return np.array([\n",
    "            [np.cos(theta), 0.0, -np.sin(theta)],\n",
    "            [0.0, 1.0, 0.0],\n",
    "            [np.sin(theta), 0.0, np.cos(theta)]\n",
    "        ])\n",
    "    # [cos(theta) sin(theta) 0; -sin(theta) cos(theta) 0; 0 0 1]\n",
    "    def Rz(theta):\n",
    "        return np.array([\n",
    "            [np.cos(theta), np.sin(theta), 0.0],\n",
    "            [-np.sin(theta), np.cos(theta), 0.0],\n",
    "            [0.0, 0.0, 1.0]])\n",
    "    \n",
    "    R = np.eye(3)\n",
    "    for idx, dim in reversed(list(enumerate(order))):\n",
    "        if dim == 'x':\n",
    "            R_func = Rx\n",
    "        elif dim == 'y':\n",
    "            R_func = Ry\n",
    "        elif dim == 'z':\n",
    "            R_func = Rz\n",
    "            \n",
    "        if idx == 0:\n",
    "            theta = alpha\n",
    "        elif idx == 1:\n",
    "            theta = beta\n",
    "        elif idx == 2:\n",
    "            theta = gamma\n",
    "            \n",
    "        R = np.dot(R, R_func(theta))\n",
    "        \n",
    "    return R\n",
    "\n",
    "# https://github.com/polarch/Higher-Order-Ambisonics/blob/master/rotateBformat.m\n",
    "def rotate_bformat(bfsig, yaw, pitch, roll, order='xyz'):\n",
    "    R_mat = euler_to_rotation_matrix(-yaw*np.pi/180.0,\n",
    "                                     -pitch*np.pi/180.0,\n",
    "                                     roll*np.pi/180,\n",
    "                                     order=order);\n",
    "\n",
    "    # augment with zero order\n",
    "    Rbf = np.zeros((4,4));\n",
    "    Rbf[0,:] = 1.0;\n",
    "    Rbf[1:,1:] = R_mat;\n",
    "\n",
    "    # apply to B-format signals\n",
    "    return np.dot(Rbf, bfsig)\n",
    "\n",
    "def rotate_coord(coord, yaw, pitch, roll, order='xyz'):\n",
    "    R_mat = euler_to_rotation_matrix(-yaw*np.pi/180.0,\n",
    "                                     -pitch*np.pi/180.0,\n",
    "                                     roll*np.pi/180,\n",
    "                                     order=order);\n",
    "\n",
    "    return np.dot(R_mat, coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "azi_list = []\n",
    "elv_list = []\n",
    "pos_list = []\n",
    "\n",
    "for grid_x in np.arange(13,step=1):\n",
    "    for grid_y in np.arange(13, step=1):\n",
    "        # Note that we swap x and y here to be consistent with standard \n",
    "        # spherical coordinate convention\n",
    "        x = grid_y\n",
    "        y = grid_x\n",
    "        mic_coord = np.array([x, y, 0])\n",
    "        for d_yaw in np.arange(-180, 180, 60):\n",
    "            for d_pitch in np.arange(-90, 90 + 10, 45):\n",
    "                for d_roll in np.arange(-90, 90 + 10, 45):\n",
    "\n",
    "\n",
    "                    rel_src_coord = mic_coord - speaker_coord\n",
    "                    rot_src_coord = rotate_coord(rel_src_coord, d_yaw, d_pitch, d_roll, order='xyz')\n",
    "                    rot_src_coord_spherical = cartesian_to_spherical(rot_src_coord, rads=False)\n",
    "                    rot_azi, rot_elv = rot_src_coord_spherical[1:]\n",
    "\n",
    "                    azi_list.append(rot_azi)\n",
    "                    elv_list.append(rot_elv)\n",
    "                    pos_list.append((x, y, d_yaw, d_pitch, d_roll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_to_pos_dict = {}\n",
    "for azi, elv, pos in zip(azi_list, elv_list, pos_list):\n",
    "    if (azi, elv) not in sc_to_pos_dict:\n",
    "        sc_to_pos_dict[(azi, elv)] = []\n",
    "    sc_to_pos_dict[(azi, elv)].append(pos)\n",
    "    \n",
    "sc_list = sorted(sorted(list(sc_to_pos_dict.keys()), key=lambda x: x[1]), key=lambda x: x[0])\n",
    "azi_list, elv_list = zip(*sc_list)\n",
    "del pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate matrix of steering vectors that include azimuth & elevation of certain range\n",
    "def steer_vector(azis,eles):\n",
    "    #azi_res is resolution of azimuth angle from -180 to 180\n",
    "    #ele_res is resolution of elevation angle from -90 to 90\n",
    "    #theta=np.arange(-180,180,azi_res)\n",
    "    #phi=np.arange(-90,90,ele_res)\n",
    "    \n",
    "    #azis and eles are pairs of chosen directions\n",
    "    m=azis.shape[0]\n",
    "    n=eles.shape[0]\n",
    "    D=np.zeros((4,m*n))#the steering matrix is of size (4,len(pairs)))\n",
    "  \n",
    "    for idx, azi in enumerate(azis):\n",
    "        for idx2, ele in enumerate(eles):            \n",
    "            d=np.array([1,np.sqrt(3)*np.cos(azi)*np.cos(ele),np.sqrt(3)*np.sin(azi)*np.cos(ele),np.sqrt(3)*np.sin(ele)])\n",
    "            D[:,idx*n+idx2]=d\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple anechoic beamformer is the pseudo inverse of steering matrix.\n",
    "def beamformer(pair,steer_mat):\n",
    "    #pair is the index of desired pair of azimuth/elevation, D=(m,n), inv(D)=(n,m)\n",
    "    u=np.zeros(steer_mat.shape[1])#(1,n)\n",
    "    u[pair]=1\n",
    "    beamformer=np.linalg.pinv(steer_mat)*u[:,None]#output=(n,m) should compute it only once\n",
    "    return beamformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that given azi,ele info, output index in the steering matrix, subject to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index of the position pair (azi, ele) in the steering matrix\n",
    "#def pairidx(azi,ele):\n",
    "#    return int((azi+180)/30*11+(ele+50)/10)\n",
    "\n",
    "#pair_idx=pairidx(0,0)\n",
    "#bf=beamformer(pair_idx,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurematrix(tgt_idx,clip,D):\n",
    "    #azi and ele are two numbers corresponding to position of targeted speech\n",
    "    #clip is the synthesized 4-channel audio clip\n",
    "    #D is the steering matrix, which is constant for all calculations\n",
    "    clip_w=clip[0,:]\n",
    "    bf=beamformer(tgt_idx,D)\n",
    "    bf_tgt=bf[tgt_idx,:]#1 by 4 vector\n",
    "    #compute stft of 4 channels of audioclip\n",
    "    x_sp_w=signal.stft(clip[0,:], fs=16e3, window=('bohman',1024), nperseg=1024, noverlap=None)#also xw\n",
    "    x_sp_x=signal.stft(clip[1,:], fs=16e3, window=('bohman',1024), nperseg=1024, noverlap=None)\n",
    "    x_sp_y=signal.stft(clip[2,:], fs=16e3, window=('bohman',1024), nperseg=1024, noverlap=None)\n",
    "    x_sp_z=signal.stft(clip[3,:], fs=16e3, window=('bohman',1024), nperseg=1024, noverlap=None)\n",
    "    x_sp=np.stack((x_sp_w,x_sp_x,x_sp_y,x_sp_z), axis=2)#dimension should be (#time frame, #freq bin,4)\n",
    "    #s_hat should be (#time frames, #frenquency bins)\n",
    "    s_hat=np.abs(np.sum(bf_tgt[None,None,:]*x_sp,axis=2))# this part remains a question, broadcasting not equivalent to matrix multiplication\n",
    "    #     #n_hat should be (#time frames, #frequency bins, #however many other directions we count as distraction)\n",
    "    #     #problematic!!\n",
    "    #     dis_idx=random.randint(0,D.shape[0]-1)#for now just pick a random direction\n",
    "    #     bf_dis=bf[dis_idx,:]\n",
    "    #     n_hat=np.sum(bf_dis[None,None,:]*x_sp,axis=2)\n",
    "    #the final concatenated feature (#time frames, 3*#frequency bins)\n",
    "    feature=np.stack((x_sp_w,s_hat),axis=1)\n",
    "    return feature\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.ones((2,3,4))*0.5\n",
    "b=np.ones((2,3,4))*(-0.5)\n",
    "print(np.sum((a*b),axis=2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Masks Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. compute mask from speech and noise signals in w channel\n",
    "2. obtain multichannel weiner filter(MWF) using GEVD approach to replace common mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input src and noise should be only in the W channel\n",
    "def compute_masks(src, noise):\n",
    "    #assuming each audio clip is sampled at 16kHz,compute the STFT\n",
    "    #with a sinusoidal window of 1024 samples and 50% overlap.\n",
    "    #window=signal.get_window('bohman',1024)\n",
    "    sw=signal.stft(src, fs=16e3, window=('bohman',1024), nperseg=1024, noverlap=None)#need to check dimensions of these\n",
    "    nw=signal.stft(noise, fs=16e3, window=('bohman',1024), nperseg=1024, noverlap=None)\n",
    "    Ms=sw*sw/(sw*sw+nw*nw)#mask should be (#time frame, #frequency bin) is it point-wise multiplication?\n",
    "    Mn=1-Ms\n",
    "    return Ms,Mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute speech s_hat from mask, then covariance matrix PHI_ss/PHI_nn from s_hat, then PHI_ss-r1/PHI_nn-r1, then wGEVD\n",
    "\n",
    "def get_GEVD(Mask_s,Mask_n,mix,speech,noise):\n",
    "    #Mask_s,Mask_n, mix, speech, noise are all of size (#time, #freq)\n",
    "    s_bar=Mask_s*mix#pointwise multiplication\n",
    "    T,F=s_bar.shape\n",
    "    phi_ss=1/T*np.sum(s_bar*s_bar,axis=0)#should be of (#freq,)\n",
    "    #phi_ss=1/T*np.matrix(np.sum(s_hat*s_hat.conjugate().transpose(),axis=0))#the dimension is not #freq then\n",
    "    sn_bar==Mask_n*mix\n",
    "    Tn,Fn=sn_bar.shape\n",
    "    phi_nn=1/Tn*np.sum(sn_bar*sn_bar,axis=0)\n",
    "    #phi_nn=1/Tn*np.matrix(np.sum(sn_hat*sn_hat.conjugate().transpose(),axis=0))\n",
    "    #rank-1 approximation\n",
    "    u,s,v=np.linalg.svd(phi_ss, full_matrices=False)\n",
    "    phi_ss_r1=s[0] * np.outer(u[0], v[0])#u need\n",
    "    un,sn,vn=np.linalg.svd(phi_nn, full_matrices=False)\n",
    "    phi_nn_r1=sn[0]* np.outer(un[0],vn[0])\n",
    "    #get wGEVD\n",
    "    u1=np.zeros((phi_nn_r1.shape[0]))\n",
    "    u1[0]=1\n",
    "    wGEVD=np.matmul(inv(phi_ss_r1+phi_nn_r1),phi_ss_r1)*u1#what is u1 in this case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass MWF back to mixture and reconstruct desired signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
