{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the customized feature extraction function that is going to be wrapped in streamer later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def noisy_samples(X, Y, sigma=1.0):\n",
    "    '''Generate an infinite stream of noisy samples from a labeled dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape=(d,)\n",
    "        Features\n",
    "\n",
    "    Y : np.ndarray, shape=(,)\n",
    "        Labels\n",
    "\n",
    "    sigma : float > 0\n",
    "        Variance of the additive noise\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    sample : dict\n",
    "        sample['X'] is an `np.ndarray` of shape `(d,)`\n",
    "\n",
    "        sample['Y'] is a scalar `np.ndarray` of shape `(,)`\n",
    "    '''\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    while True:\n",
    "        i = np.random.randint(0, n)\n",
    "\n",
    "        noise = sigma * np.random.randn(1, d)\n",
    "\n",
    "        yield dict(X=X[i] + noise, Y=Y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate over random batches among the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pescador\n",
    "\n",
    "streamer = pescador.Streamer(noisy_samples, X[train], Y[train])\n",
    "#get one batch\n",
    "stream2 = streamer.iterate()\n",
    "# Equivalent to stream2 above\n",
    "stream3 = streamer()\n",
    "\n",
    "\n",
    "#to iterate over all the batches\n",
    "for sample in streamer.iterate():\n",
    "    # do something\n",
    "    ...\n",
    "\n",
    "# For convenience, the object directly behaves as an iterator.\n",
    "for sample in streamer:\n",
    "    # do something\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_data():\n",
    "    \"\"\"Load and shape data for training with Keras + Pescador.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    input_shape : tuple, len=3\n",
    "        Shape of each sample; adapts to channel configuration of Keras.\n",
    "\n",
    "    X_train, y_train : np.ndarrays\n",
    "        Images and labels for training.\n",
    "\n",
    "    X_test, y_test : np.ndarrays\n",
    "        Images and labels for test.\n",
    "    \"\"\"\n",
    "    # The data, shuffled and split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return input_shape, (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"Create a compiled Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : tuple, len=3\n",
    "        Shape of each image sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : keras.Model\n",
    "        Constructed model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler(X, y):\n",
    "    '''A basic generator for sampling data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, len=n_samples, ndim=4\n",
    "        Image data.\n",
    "\n",
    "    y : np.ndarray, len=n_samples, ndim=2\n",
    "        One-hot encoded class vectors.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    data : dict\n",
    "        Single image sample, like {X: np.ndarray, y: np.ndarray}\n",
    "    '''\n",
    "    X = np.atleast_2d(X)\n",
    "    # y's are binary vectors, and should be of shape (10,) after this.\n",
    "    y = np.atleast_1d(y)\n",
    "\n",
    "    n = X.shape[0]\n",
    "\n",
    "    while True:\n",
    "        i = np.random.randint(0, n)\n",
    "        yield {'X': X[i], 'y': y[i]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additive_noise(stream, key='X', scale=1e-1):\n",
    "    '''Add noise to a data stream.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stream : iterable\n",
    "        A stream that yields data objects.\n",
    "\n",
    "    key : string, default='X'\n",
    "        Name of the field to add noise.\n",
    "\n",
    "    scale : float, default=0.1\n",
    "        Scale factor for gaussian noise.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    data : dict\n",
    "        Updated data objects in the stream.\n",
    "    '''\n",
    "    for data in stream:\n",
    "        noise_shape = data[key].shape\n",
    "        noise = scale * np.random.randn(*noise_shape)\n",
    "        data[key] = data[key] + noise\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape, (X_train, Y_train), (X_test, Y_test) = setup_data()\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "# Create two streams from the same data, where one of the streams\n",
    "# adds a small amount of Gaussian noise. You could easily perform\n",
    "# other data augmentations using the same 'map' strategy.\n",
    "stream = pescador.Streamer(sampler, X_train, Y_train)\n",
    "noisy_stream = pescador.Streamer(additive_noise, stream, 'X')\n",
    "\n",
    "# Multiplex the two streamers together.\n",
    "mux = pescador.StochasticMux([stream, noisy_stream],\n",
    "                             # Two streams, always active.\n",
    "                             n_active=2,\n",
    "                             # We want to sample from each stream infinitely.\n",
    "                             rate=None)\n",
    "\n",
    "# Buffer the stream into minibatches.\n",
    "batches = pescador.buffer_stream(mux, batch_size)\n",
    "\n",
    "model = build_model(input_shape)\n",
    "try:\n",
    "    print(\"Start time: {}\".format(datetime.datetime.now()))\n",
    "    model.fit_generator(\n",
    "        pescador.tuples(batches, 'X', 'y'),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_test, Y_test))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping early\")\n",
    "finally:\n",
    "    print(\"Finished: {}\".format(datetime.datetime.now()))\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    for val, name in zip(scores, model.metrics_names):\n",
    "        print('Test {}: {:0.4f}'.format(name, val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
